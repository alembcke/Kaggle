{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/alembcke/titanic-multi-layer-perceptron-using-haiku-jax?scriptVersionId=103649732\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","id":"ca89eeb6","metadata":{"papermill":{"duration":0.009439,"end_time":"2022-08-18T22:43:45.961743","exception":false,"start_time":"2022-08-18T22:43:45.952304","status":"completed"},"tags":[]},"source":["# Titanic Multi-Layer Perceptron using Haiku/JAX\n","\n","*by Alex Lembcke*\n","\n","*August 2022*\n","\n","This notebook demonstrates how to build a simple multi-layer perceptron neural network using [Haiku](https://github.com/deepmind/dm-haiku) and [JAX](https://github.com/google/jax).  We will use the Titanic dataset as part of the introductory Kaggle competition [Titanic - Machine Learning from Disaster](https://www.kaggle.com/competitions/titanic).  The goal of this competition is to predict whether an individual aboard the Titanic survived its sinking on April 15, 1912.  Competitors are given a training dataset containing various data on 891 passengers, including whether or not they survived.  They must then provide their predictions for survival for 418 other passengers, given the same information as was provided in the training dataset, except whether the passenger survived, of course.\n","\n","As Haiku is not install by default in Kaggle, so we will first need to install it:"]},{"cell_type":"code","execution_count":1,"id":"d6d2bf1c","metadata":{"execution":{"iopub.execute_input":"2022-08-18T22:43:45.980074Z","iopub.status.busy":"2022-08-18T22:43:45.979322Z","iopub.status.idle":"2022-08-18T22:44:00.416038Z","shell.execute_reply":"2022-08-18T22:44:00.414763Z"},"papermill":{"duration":14.449334,"end_time":"2022-08-18T22:44:00.419091","exception":false,"start_time":"2022-08-18T22:43:45.969757","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting dm-haiku\r\n","  Downloading dm_haiku-0.0.7-py3-none-any.whl (342 kB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.4/342.4 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hRequirement already satisfied: absl-py>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from dm-haiku) (1.1.0)\r\n","Requirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.7/site-packages (from dm-haiku) (0.8.10)\r\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from dm-haiku) (4.1.1)\r\n","Collecting jmp>=0.0.2\r\n","  Downloading jmp-0.0.2-py3-none-any.whl (16 kB)\r\n","Requirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.7/site-packages (from dm-haiku) (1.21.6)\r\n","Installing collected packages: jmp, dm-haiku\r\n","Successfully installed dm-haiku-0.0.7 jmp-0.0.2\r\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n","\u001b[0m"]}],"source":["!pip install --upgrade dm-haiku"]},{"cell_type":"markdown","id":"a7b30573","metadata":{"papermill":{"duration":0.008334,"end_time":"2022-08-18T22:44:00.437062","exception":false,"start_time":"2022-08-18T22:44:00.428728","status":"completed"},"tags":[]},"source":["Now we can import Haiku and JAX, as well as some other libraries that will help us along the way:"]},{"cell_type":"code","execution_count":2,"id":"9f5cfde0","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-08-18T22:44:00.457096Z","iopub.status.busy":"2022-08-18T22:44:00.455922Z","iopub.status.idle":"2022-08-18T22:44:03.03617Z","shell.execute_reply":"2022-08-18T22:44:03.034758Z"},"papermill":{"duration":2.593184,"end_time":"2022-08-18T22:44:03.038894","exception":false,"start_time":"2022-08-18T22:44:00.44571","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["JAX version 0.3.14\n","Haiku version 0.0.7\n"]}],"source":["# Typing library to hold training state\n","from typing import NamedTuple\n","\n","# Libraries for input/output and data manipulation\n","import pandas as pd\n","import numpy as np\n","\n","# Make numpy values easier to read\n","np.set_printoptions(precision=3, suppress=True)\n","\n","# Machine learning libraries\n","import haiku as hk\n","import jax\n","import jax.numpy as jnp\n","\n","# Optimization library\n","import optax\n","\n","# Print version numbers\n","print(\"JAX version {}\".format(jax.__version__))\n","print(\"Haiku version {}\".format(hk.__version__))"]},{"cell_type":"markdown","id":"25ff99c0","metadata":{"papermill":{"duration":0.008249,"end_time":"2022-08-18T22:44:03.056034","exception":false,"start_time":"2022-08-18T22:44:03.047785","status":"completed"},"tags":[]},"source":["## Data Cleansing\n","\n","Unfortunately, or perhaps to add an element of realism to this Kaggle competition, the dataset provided is not entirely perfect.  But there is a whole host of notebooks detailing the issues in the dataset and how to solve them.  And since the goal of this notebook is to demonstrate how to implement a neural network using Haiku and JAX, we will simply clean the data and move on.  First, we must retrieve the datasets:"]},{"cell_type":"code","execution_count":3,"id":"576db1a7","metadata":{"execution":{"iopub.execute_input":"2022-08-18T22:44:03.075733Z","iopub.status.busy":"2022-08-18T22:44:03.074801Z","iopub.status.idle":"2022-08-18T22:44:03.105561Z","shell.execute_reply":"2022-08-18T22:44:03.104324Z"},"papermill":{"duration":0.043721,"end_time":"2022-08-18T22:44:03.108449","exception":false,"start_time":"2022-08-18T22:44:03.064728","status":"completed"},"tags":[]},"outputs":[],"source":["train = pd.read_csv(\"../input/titanic/train.csv\")\n","test = pd.read_csv(\"../input/titanic/test.csv\")"]},{"cell_type":"markdown","id":"d87dcf06","metadata":{"papermill":{"duration":0.008431,"end_time":"2022-08-18T22:44:03.125864","exception":false,"start_time":"2022-08-18T22:44:03.117433","status":"completed"},"tags":[]},"source":["One of the data issues is that the Embarked field is missing two data points in the training dataset.  Since it is only two data points, we can check online to confirm that both of those passengers boarded at Southhampton and fill in the missing information:"]},{"cell_type":"code","execution_count":4,"id":"63ba25e4","metadata":{"execution":{"iopub.execute_input":"2022-08-18T22:44:03.14521Z","iopub.status.busy":"2022-08-18T22:44:03.14444Z","iopub.status.idle":"2022-08-18T22:44:03.155249Z","shell.execute_reply":"2022-08-18T22:44:03.154209Z"},"papermill":{"duration":0.023099,"end_time":"2022-08-18T22:44:03.157684","exception":false,"start_time":"2022-08-18T22:44:03.134585","status":"completed"},"tags":[]},"outputs":[],"source":["train['Embarked'].fillna('S', inplace=True)"]},{"cell_type":"markdown","id":"d70de5ff","metadata":{"papermill":{"duration":0.009064,"end_time":"2022-08-18T22:44:03.175496","exception":false,"start_time":"2022-08-18T22:44:03.166432","status":"completed"},"tags":[]},"source":["Next is the big issue: the Age field is missing quite a lot of information in both the training dataset and the testing dataset.  Given how much of the data is missing and how big of an impact it could make on the final predictions, it makes sense to fill in this data as accurately as possible - although it is too much to verify by hand.  One solution proposed by a number of Kagglers is to fill in the age field by inferring the value given the title of the individual (from the Name field) and for those with the title of \"Miss\" to add in whether they are traveling with a parent or not.\n","\n","We will use the solution provided in the notebook [Titanic Missing Age Imputation Tutorial - Advanced](https://www.kaggle.com/code/allohvk/titanic-missing-age-imputation-tutorial-advanced/notebook) to fill in the missing ages:"]},{"cell_type":"code","execution_count":5,"id":"a8edf182","metadata":{"execution":{"iopub.execute_input":"2022-08-18T22:44:03.194586Z","iopub.status.busy":"2022-08-18T22:44:03.194157Z","iopub.status.idle":"2022-08-18T22:44:03.519488Z","shell.execute_reply":"2022-08-18T22:44:03.518234Z"},"papermill":{"duration":0.338392,"end_time":"2022-08-18T22:44:03.522357","exception":false,"start_time":"2022-08-18T22:44:03.183965","status":"completed"},"tags":[]},"outputs":[],"source":["# Create the Title field and fill it in\n","train['Title'], test['Title'] = [df.Name.str.extract (' ([A-Za-z]+)\\.', expand=False) for df in [train, test]]\n","TitleDict = {\"Capt\": \"Officer\",\"Col\": \"Officer\",\"Major\": \"Officer\",\"Jonkheer\": \"Royalty\", \\\n","             \"Don\": \"Royalty\", \"Sir\" : \"Royalty\",\"Dr\": \"Royalty\",\"Rev\": \"Royalty\", \\\n","             \"Countess\":\"Royalty\", \"Mme\": \"Mrs\", \"Mlle\": \"Miss\", \"Ms\": \"Mrs\",\"Mr\" : \"Mr\", \\\n","             \"Mrs\" : \"Mrs\",\"Miss\" : \"Miss\",\"Master\" : \"Master\",\"Lady\" : \"Royalty\"}\n","train['Title'], test['Title'] = [df.Title.map(TitleDict) for df in [train, test]]\n","# Create a table showing the mean age per age group\n","grp = train.groupby(['Pclass','Sex','Title'])['Age'].mean().reset_index()[['Sex', 'Pclass', 'Title', 'Age']]\n","\n","def fill_age(x):\n","    \"\"\"Fills in the missing values for the Age field.\"\"\"\n","    return grp[(grp.Pclass==x.Pclass)&(grp.Sex==x.Sex)&(grp.Title==x.Title)]['Age'].values[0]\n","train['Age'], test['Age'] = [df.apply(lambda x: fill_age(x) if np.isnan(x['Age']) else x['Age'], axis=1) for df in [train, test]]"]},{"cell_type":"markdown","id":"2ffd6eb8","metadata":{"papermill":{"duration":0.00878,"end_time":"2022-08-18T22:44:03.540551","exception":false,"start_time":"2022-08-18T22:44:03.531771","status":"completed"},"tags":[]},"source":["While this solution does present the issue of data leakage from training dataset to testing dataset, by filling in the data in the testing dataset using values imputed from the training dataset, given the goal of this notebook and that is an introductory competition, we will simply note this is not a good idea and continue forward.\n","\n","Now we have (fairly) clean training and testing datasets."]},{"cell_type":"markdown","id":"c24a445e","metadata":{"papermill":{"duration":0.008513,"end_time":"2022-08-18T22:44:03.558239","exception":false,"start_time":"2022-08-18T22:44:03.549726","status":"completed"},"tags":[]},"source":["## Data Pipeline\n","\n","As our goal is to setup a multi-layer perceptron using Haiku/JAX, we will skip the exploratory data analysis step and go straight to building our model - starting with building the data pipeline.  We will start by choosing which features to use in our model.  We can drop the `Title` field we created to impute the age, as well as the `Cabin` and `Fare` fields as they require a significant amount of feature engineering to provide value, and again that is not the stated goal of this notebook."]},{"cell_type":"code","execution_count":6,"id":"8f32edb5","metadata":{"execution":{"iopub.execute_input":"2022-08-18T22:44:03.577902Z","iopub.status.busy":"2022-08-18T22:44:03.577058Z","iopub.status.idle":"2022-08-18T22:44:03.585261Z","shell.execute_reply":"2022-08-18T22:44:03.584418Z"},"papermill":{"duration":0.02052,"end_time":"2022-08-18T22:44:03.58747","exception":false,"start_time":"2022-08-18T22:44:03.56695","status":"completed"},"tags":[]},"outputs":[],"source":["# Setup training data\n","titanic_features = train[['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Embarked']]\n","titanic_labels = titanic_features.pop('Survived')\n","\n","# Setup testing data\n","titanic_test = test[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Embarked']]"]},{"cell_type":"markdown","id":"f7cb25f0","metadata":{"papermill":{"duration":0.008288,"end_time":"2022-08-18T22:44:03.604485","exception":false,"start_time":"2022-08-18T22:44:03.596197","status":"completed"},"tags":[]},"source":["Next we need to convert categorical data into one-hot vectors and normalize numerical data.  The two categorical fields are `Sex` and `Embarked`, while the remainder of the fields are numerical.  Then we can merge the fields back together to get something suitable as input for a neural network.\n","\n","It should be noted that there is a lot of feature engineering that could be done here, and which others have demonstrated.  For example, `Age` would likely be better when broken into buckets, rather than left as a numerical input.  Also, the fields `SibSp` and `Parch` would be better used to match families together.  But we will focus on our goal of providing a simplified demonstration of a neural network using Haiku and JAX and leave the advanced feature engineering as an exercise for the reader (that's you)."]},{"cell_type":"code","execution_count":7,"id":"01b9935d","metadata":{"execution":{"iopub.execute_input":"2022-08-18T22:44:03.624121Z","iopub.status.busy":"2022-08-18T22:44:03.623076Z","iopub.status.idle":"2022-08-18T22:44:04.880958Z","shell.execute_reply":"2022-08-18T22:44:04.879818Z"},"papermill":{"duration":1.270613,"end_time":"2022-08-18T22:44:04.883783","exception":false,"start_time":"2022-08-18T22:44:03.61317","status":"completed"},"tags":[]},"outputs":[],"source":["def preprocess(dataset):\n","    \"\"\"Preprocesses the inputs, transforming categorical inputs to one-hot vectors and\n","    normalizes numerical inputs. Combines and returns result.\"\"\"\n","    # Convert categorical data to one-hot vectors\n","    sex_numeric = dataset['Sex'].map( {'male': 0, 'female': 1} ).astype(int).to_numpy()\n","    sex_one_hot = jax.nn.one_hot(sex_numeric, num_classes=2)\n","    embarked_numeric = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int).to_numpy()\n","    embarked_one_hot = jax.nn.one_hot(embarked_numeric, num_classes=3)\n","    # Normalize numeric inputs\n","    numeric_inputs = dataset[['Pclass', 'Age', 'SibSp', 'Parch']]\n","    normalized_inputs = []\n","    for feature in numeric_inputs:\n","        norm = jnp.linalg.norm(numeric_inputs[feature].to_numpy())\n","        normalized_inputs.append(numeric_inputs[feature].to_numpy() / norm)\n","    # Append all of the inputs together\n","    return jnp.array([sex_one_hot[:,0],\n","                      sex_one_hot[:,1],\n","                      embarked_one_hot[:,0],\n","                      embarked_one_hot[:,1],\n","                      embarked_one_hot[:,2],\n","                      normalized_inputs[0],\n","                      normalized_inputs[1],\n","                      normalized_inputs[2],\n","                      normalized_inputs[3]\n","                     ]).T\n","\n","titanic_features = preprocess(titanic_features)\n","titanic_labels = jnp.array(titanic_labels.to_numpy()).reshape((titanic_features.shape[0], 1))\n","\n","titanic_test = preprocess(titanic_test)"]},{"cell_type":"markdown","id":"501b5b4e","metadata":{"papermill":{"duration":0.008278,"end_time":"2022-08-18T22:44:04.90083","exception":false,"start_time":"2022-08-18T22:44:04.892552","status":"completed"},"tags":[]},"source":["Now our data is suitable to be passed into a neural network, so time to get to the fun part!"]},{"cell_type":"markdown","id":"9d49bfcd","metadata":{"papermill":{"duration":0.008246,"end_time":"2022-08-18T22:44:04.917788","exception":false,"start_time":"2022-08-18T22:44:04.909542","status":"completed"},"tags":[]},"source":["## Multi-Layer Perceptron Model\n","\n","Now that we cleaned and preprocessed our data, it is time to build our model.  We will start by declaring a class to hold our training state.  The training state will keep track of the trained parameters, an exponential average of the trained parameters and our optimizer state:"]},{"cell_type":"code","execution_count":8,"id":"a5510d63","metadata":{"execution":{"iopub.execute_input":"2022-08-18T22:44:04.936727Z","iopub.status.busy":"2022-08-18T22:44:04.936296Z","iopub.status.idle":"2022-08-18T22:44:04.94131Z","shell.execute_reply":"2022-08-18T22:44:04.940423Z"},"papermill":{"duration":0.016911,"end_time":"2022-08-18T22:44:04.943332","exception":false,"start_time":"2022-08-18T22:44:04.926421","status":"completed"},"tags":[]},"outputs":[],"source":["class TrainingState(NamedTuple):\n","    params: hk.Params\n","    avg_params: hk.Params\n","    opt_state: optax.OptState"]},{"cell_type":"markdown","id":"c598e528","metadata":{"papermill":{"duration":0.00824,"end_time":"2022-08-18T22:44:04.960207","exception":false,"start_time":"2022-08-18T22:44:04.951967","status":"completed"},"tags":[]},"source":["Next, we will define our network.  As mentioned in the title, we will use a simple multi-layer perceptron model, which was taken from the [MNIST Example](https://github.com/deepmind/dm-haiku/blob/main/examples/mnist.py) provided on the GitHub page for the Haiku library - in fact, much of this code was adapted from that example, with modifications as required.  But as that example is for a classification problem, and we are solving a probability problem, the network was adjusted to output a probability by adding a sigmoid activation function to the last layer, and reducing that layer to one neuron:"]},{"cell_type":"code","execution_count":9,"id":"26eb20a3","metadata":{"execution":{"iopub.execute_input":"2022-08-18T22:44:04.980795Z","iopub.status.busy":"2022-08-18T22:44:04.980013Z","iopub.status.idle":"2022-08-18T22:44:04.986049Z","shell.execute_reply":"2022-08-18T22:44:04.985077Z"},"papermill":{"duration":0.018729,"end_time":"2022-08-18T22:44:04.988583","exception":false,"start_time":"2022-08-18T22:44:04.969854","status":"completed"},"tags":[]},"outputs":[],"source":["def net_fn(features: jnp.ndarray) -> jnp.ndarray:\n","    \"\"\"Standard LeNet-300-100 MLP network.\"\"\"\n","    mlp = hk.Sequential([\n","        hk.Flatten(),\n","        hk.Linear(300), jax.nn.relu,\n","        hk.Linear(100), jax.nn.relu,\n","        hk.Linear(1), jax.nn.sigmoid\n","    ])\n","    return mlp(features)"]},{"cell_type":"markdown","id":"24e145f2","metadata":{"papermill":{"duration":0.008465,"end_time":"2022-08-18T22:44:05.005909","exception":false,"start_time":"2022-08-18T22:44:04.997444","status":"completed"},"tags":[]},"source":["Now we need to create an instance our model and optimizer, for which we will use the Adam algorithm:"]},{"cell_type":"code","execution_count":10,"id":"a297ed15","metadata":{"execution":{"iopub.execute_input":"2022-08-18T22:44:05.025468Z","iopub.status.busy":"2022-08-18T22:44:05.024736Z","iopub.status.idle":"2022-08-18T22:44:05.029697Z","shell.execute_reply":"2022-08-18T22:44:05.02873Z"},"papermill":{"duration":0.017254,"end_time":"2022-08-18T22:44:05.031862","exception":false,"start_time":"2022-08-18T22:44:05.014608","status":"completed"},"tags":[]},"outputs":[],"source":["network = hk.without_apply_rng(hk.transform(net_fn))\n","optimiser = optax.adam(1e-3)"]},{"cell_type":"markdown","id":"0834c41f","metadata":{"papermill":{"duration":0.008359,"end_time":"2022-08-18T22:44:05.04901","exception":false,"start_time":"2022-08-18T22:44:05.040651","status":"completed"},"tags":[]},"source":["As the problem we are solving is one of probability, the chance of survival aboard the Titanic, we will use  binary cross-entropy loss:"]},{"cell_type":"code","execution_count":11,"id":"7d33f914","metadata":{"execution":{"iopub.execute_input":"2022-08-18T22:44:05.069307Z","iopub.status.busy":"2022-08-18T22:44:05.068552Z","iopub.status.idle":"2022-08-18T22:44:05.073737Z","shell.execute_reply":"2022-08-18T22:44:05.072795Z"},"papermill":{"duration":0.018233,"end_time":"2022-08-18T22:44:05.07588","exception":false,"start_time":"2022-08-18T22:44:05.057647","status":"completed"},"tags":[]},"outputs":[],"source":["def binary_cross_entropy(logits: jnp.ndarray, labels: jnp.ndarray, epsilon):\n","    return labels * jnp.log(logits + epsilon) + (1 - labels)*jnp.log(1 - logits + epsilon)"]},{"cell_type":"markdown","id":"41d71c0f","metadata":{"papermill":{"duration":0.008384,"end_time":"2022-08-18T22:44:05.093032","exception":false,"start_time":"2022-08-18T22:44:05.084648","status":"completed"},"tags":[]},"source":["We will add gradient clipping to our loss function to avoid vanishing/exploding gradients, a problem that did arise when the network was run without gradient clipping.  If you are new to machine learning and want to see what happens when we don't clip the gradients, then set `epsilon = 0` below and rerun the notebook - you should notice a problem when running the training loop a few steps later."]},{"cell_type":"code","execution_count":12,"id":"340a8f6a","metadata":{"execution":{"iopub.execute_input":"2022-08-18T22:44:05.112764Z","iopub.status.busy":"2022-08-18T22:44:05.111991Z","iopub.status.idle":"2022-08-18T22:44:05.118208Z","shell.execute_reply":"2022-08-18T22:44:05.117301Z"},"papermill":{"duration":0.018833,"end_time":"2022-08-18T22:44:05.12062","exception":false,"start_time":"2022-08-18T22:44:05.101787","status":"completed"},"tags":[]},"outputs":[],"source":["def loss(params: hk.Params, features: jnp.ndarray, labels: jnp.ndarray):\n","    \"\"\"Binary cross entropy loss with gradient clipping.\"\"\"\n","    epsilon = 1e-7\n","    jnp.clip(labels, epsilon, 1-epsilon)\n","    m = labels.shape[1]\n","    logits = network.apply(params, features)\n","    return -1/m * jnp.mean(binary_cross_entropy(logits, labels, epsilon))"]},{"cell_type":"markdown","id":"1bba4c02","metadata":{"papermill":{"duration":0.008463,"end_time":"2022-08-18T22:44:05.13778","exception":false,"start_time":"2022-08-18T22:44:05.129317","status":"completed"},"tags":[]},"source":["Then we will create our `evaluate` function, which we will use to keep track of how our model is performing in the training stage:"]},{"cell_type":"code","execution_count":13,"id":"62d8073e","metadata":{"execution":{"iopub.execute_input":"2022-08-18T22:44:05.157217Z","iopub.status.busy":"2022-08-18T22:44:05.15625Z","iopub.status.idle":"2022-08-18T22:44:05.162177Z","shell.execute_reply":"2022-08-18T22:44:05.161306Z"},"papermill":{"duration":0.018068,"end_time":"2022-08-18T22:44:05.164428","exception":false,"start_time":"2022-08-18T22:44:05.14636","status":"completed"},"tags":[]},"outputs":[],"source":["@jax.jit\n","def evaluate(params: hk.Params, features: jnp.ndarray, labels: jnp.ndarray):\n","    \"\"\"Checks the accuracy of predictions compared to labels.\"\"\"\n","    logits = network.apply(params, features)\n","    predictions = jnp.around(logits, 0)\n","    return jnp.mean(predictions == labels)"]},{"cell_type":"markdown","id":"7d30b1be","metadata":{"papermill":{"duration":0.00846,"end_time":"2022-08-18T22:44:05.181737","exception":false,"start_time":"2022-08-18T22:44:05.173277","status":"completed"},"tags":[]},"source":["We will also need to update our parameters as we train our model, by using `jax.grad` to calculate the gradients of our parameters and then updating them using `optax`:"]},{"cell_type":"code","execution_count":14,"id":"d2bc1df6","metadata":{"execution":{"iopub.execute_input":"2022-08-18T22:44:05.201196Z","iopub.status.busy":"2022-08-18T22:44:05.200482Z","iopub.status.idle":"2022-08-18T22:44:05.207108Z","shell.execute_reply":"2022-08-18T22:44:05.206239Z"},"papermill":{"duration":0.01891,"end_time":"2022-08-18T22:44:05.209299","exception":false,"start_time":"2022-08-18T22:44:05.190389","status":"completed"},"tags":[]},"outputs":[],"source":["@jax.jit\n","def update(state: TrainingState, features: jnp.ndarray, labels: jnp.ndarray) -> TrainingState:\n","    \"\"\"Learning rule (stochastic gradient descent).\"\"\"\n","    grads = jax.grad(loss)(state.params, features, labels)\n","    updates, opt_state = optimiser.update(grads, state.opt_state)\n","    params = optax.apply_updates(state.params, updates)\n","    # Compute avg_params, the exponential moving average of the \"live\" params.\n","    # We use this only for evaluation (cf. https://doi.org/10.1137/0330046).\n","    avg_params = optax.incremental_update(params, state.avg_params, step_size=0.001)\n","    return TrainingState(params, avg_params, opt_state)"]},{"cell_type":"markdown","id":"160fdd9f","metadata":{"papermill":{"duration":0.00832,"end_time":"2022-08-18T22:44:05.226287","exception":false,"start_time":"2022-08-18T22:44:05.217967","status":"completed"},"tags":[]},"source":["With a lot of the heavy lifting now done, we can initalize the parameters and optimizer of our model:"]},{"cell_type":"code","execution_count":15,"id":"70c843c9","metadata":{"execution":{"iopub.execute_input":"2022-08-18T22:44:05.245391Z","iopub.status.busy":"2022-08-18T22:44:05.244696Z","iopub.status.idle":"2022-08-18T22:44:06.837553Z","shell.execute_reply":"2022-08-18T22:44:06.836314Z"},"papermill":{"duration":1.605637,"end_time":"2022-08-18T22:44:06.840514","exception":false,"start_time":"2022-08-18T22:44:05.234877","status":"completed"},"tags":[]},"outputs":[],"source":["# Initialise network and optimiser; note we draw an input to get shapes.\n","initial_params = network.init(jax.random.PRNGKey(seed=42), titanic_features[0])\n","initial_opt_state = optimiser.init(initial_params)\n","state = TrainingState(initial_params, initial_params, initial_opt_state)"]},{"cell_type":"markdown","id":"5a7c0540","metadata":{"papermill":{"duration":0.00838,"end_time":"2022-08-18T22:44:06.857547","exception":false,"start_time":"2022-08-18T22:44:06.849167","status":"completed"},"tags":[]},"source":["And it is finally time to train our model.  Here will will do 10,000 training loops and call our `evaluate` function every 1,000 loops to report our accuracy against the training data:"]},{"cell_type":"code","execution_count":16,"id":"9d56d216","metadata":{"execution":{"iopub.execute_input":"2022-08-18T22:44:06.877174Z","iopub.status.busy":"2022-08-18T22:44:06.876181Z","iopub.status.idle":"2022-08-18T22:44:41.342142Z","shell.execute_reply":"2022-08-18T22:44:41.341233Z"},"papermill":{"duration":34.478067,"end_time":"2022-08-18T22:44:41.344386","exception":false,"start_time":"2022-08-18T22:44:06.866319","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["{'step': 0, 'accuracy': '0.616'}\n","{'step': 1000, 'accuracy': '0.791'}\n","{'step': 2000, 'accuracy': '0.815'}\n","{'step': 3000, 'accuracy': '0.840'}\n","{'step': 4000, 'accuracy': '0.847'}\n","{'step': 5000, 'accuracy': '0.853'}\n","{'step': 6000, 'accuracy': '0.860'}\n","{'step': 7000, 'accuracy': '0.866'}\n","{'step': 8000, 'accuracy': '0.870'}\n","{'step': 9000, 'accuracy': '0.870'}\n","{'step': 10000, 'accuracy': '0.872'}\n"]}],"source":["# Training & evaluation loop.\n","for step in range(10001):\n","    if step % 1000 == 0:\n","        # Periodically evaluate classification accuracy on training set.\n","        accuracy = np.array(evaluate(state.avg_params, titanic_features, titanic_labels)).item()\n","        print({\"step\": step, \"accuracy\": f\"{accuracy:.3f}\"})\n","\n","    # Do SGD on training examples.\n","    state = update(state, titanic_features, titanic_labels)"]},{"cell_type":"markdown","id":"2169635e","metadata":{"papermill":{"duration":0.009176,"end_time":"2022-08-18T22:44:41.36314","exception":false,"start_time":"2022-08-18T22:44:41.353964","status":"completed"},"tags":[]},"source":["With our model having been trained on the training data, our last step is to make predictions on the test data and save a file for submission to Kaggle:"]},{"cell_type":"code","execution_count":17,"id":"230eed1e","metadata":{"execution":{"iopub.execute_input":"2022-08-18T22:44:41.384047Z","iopub.status.busy":"2022-08-18T22:44:41.383227Z","iopub.status.idle":"2022-08-18T22:44:42.290027Z","shell.execute_reply":"2022-08-18T22:44:42.288735Z"},"papermill":{"duration":0.920263,"end_time":"2022-08-18T22:44:42.292832","exception":false,"start_time":"2022-08-18T22:44:41.372569","status":"completed"},"tags":[]},"outputs":[],"source":["predictions = jnp.round(network.apply(state.params, titanic_test), 0).astype(int)\n","output = pd.DataFrame(data=predictions, columns=['Survived'], index=test['PassengerId'])\n","output.to_csv('submission.csv')"]},{"cell_type":"markdown","id":"9a84e52b","metadata":{"papermill":{"duration":0.009154,"end_time":"2022-08-18T22:44:42.311494","exception":false,"start_time":"2022-08-18T22:44:42.30234","status":"completed"},"tags":[]},"source":["And that is it...for now.  I will be updating this notebook with more detailed explanations (and likely fixing some mistakes, so please let me know if you spot any)."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":69.231018,"end_time":"2022-08-18T22:44:45.896132","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-08-18T22:43:36.665114","version":"2.3.4"}},"nbformat":4,"nbformat_minor":5}